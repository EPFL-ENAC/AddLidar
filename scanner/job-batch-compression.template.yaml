apiVersion: batch/v1
kind: Job
metadata:
  name: "compression"
  namespace: "epfl-eso-addlidar-prod"
spec:
  ttlSecondsAfterFinished: 3600 # Clean up 1 hour after job completes
  activeDeadlineSeconds: 86400 # Job timeout of 24 hours
  # prettier-ignore
  completions: {{ folders|length }} # Dynamic based on folder count
  # prettier-ignore
  parallelism: {{ parallelism|default(4) }}
  backoffLimit: 0 # No retries per completion
  completionMode: Indexed
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: job-name
                      operator: In
                      values:
                        - compression
                topologyKey: "kubernetes.io/hostname"
      restartPolicy: Never
      initContainers:
        - name: "prepare-folders"
          image: "docker.io/library/bash"
          command:
            - "bash"
            - "-c"
            - |
              # Array of folders to process as [folderName, folderFingerprint] pairs
              folders=(
              {% for folder in folders %}
                "{{ folder[0] }}|{{ folder[1] }}"
              {% endfor %}
              )

              # Get the current folder based on job index
              current_folder_pair=${folders[$JOB_COMPLETION_INDEX]}

              # Extract the folder name and fingerprint from the pair
              folder_name=$(echo "$current_folder_pair" | cut -d'|' -f1)
              folder_fingerprint=$(echo "$current_folder_pair" | cut -d'|' -f2)

              # Check if folder exists and is not empty
              if [ -n "${folder_name}" ] && [ -d "/lidar/${folder_name}" ] && [ "$(ls -A /lidar/${folder_name} 2>/dev/null)" ]; then
                echo "Found valid folder: /lidar/${folder_name} (Fingerprint: ${folder_fingerprint})"
                # Write the input and output paths to a file for the main container
                echo "${folder_name}" > /data/input_path.txt
                echo "${folder_name}.tar.gz" > /data/output_path.txt
                echo "${folder_fingerprint}" > /data/folder_fingerprint.txt
                echo "true" > /data/folder_valid.txt
              else
                echo "WARNING: Folder /lidar/${folder_name} does not exist or is empty"
                echo "false" > /data/folder_valid.txt
                echo "${folder_name}" > /data/input_path.txt
              fi
          volumeMounts:
            - name: fts-addlidar
              subPath: "fts-addlidar/LiDAR"
              mountPath: "{{ orig_dir }}"
              readOnly: true
            - mountPath: /data
              name: data
          resources:
            limits:
              memory: "100Mi"
            requests:
              memory: "10Mi"
      containers:
        - name: "compression"
          image: "{{ compression_image_registry | default('ghcr.io') }}/{{ compression_image_name | default('epfl-enac/epfl-eso/addlidar/compression') }}:{{ compression_image_tag | default('latest') }}{% if compression_image_sha256 %}@sha256:{{ compression_image_sha256 }}{% endif %}"
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash", "-c"]
          args:
            - |
              # Set backend URL for API calls
              BACKEND_URL="{{ backend_url | default('http://backend-internal') }}"

              # Check if the folder is valid before proceeding
              if [ -e "/data/folder_valid.txt" ] && [ "$(cat /data/folder_valid.txt)" == "true" ]; then
                INPUT_PATH=$(cat /data/input_path.txt)
                OUTPUT_PATH=$(cat /data/output_path.txt)
                FINGERPRINT=$(cat /data/folder_fingerprint.txt)
                START_TIME=$(date +%s)
                
                echo "Processing folder: $INPUT_PATH"
                
                # Create temporary log file for capturing detailed output
                TEMP_LOG_FILE="/tmp/archive_${INPUT_PATH//\//_}_$$.log"
                
                echo "Starting archive creation - logs will be visible here and saved to: $TEMP_LOG_FILE"
                echo "==================== ARCHIVE PROCESS START ===================="
                
                # Use tee to show logs in real-time AND save to file
                # The logs will be visible when you kubectl logs or kubectl exec into the pod
                if /usr/local/bin/archive_one_folder.sh /lidar/"$INPUT_PATH" /zips/"$OUTPUT_PATH" 2>&1 | tee "$TEMP_LOG_FILE"; then
                  echo "==================== ARCHIVE PROCESS SUCCESS ==================="
                  echo "Archive created successfully: $OUTPUT_PATH"
                  
                  # Update database with success status via API
                  echo "Updating database for ${INPUT_PATH} with success status"
                  END_TIME=$(date +%s)
                  PROCESSING_TIME=$((END_TIME - START_TIME))
                  
                  curl -X PUT "${BACKEND_URL}/sqlite/folder_state/${INPUT_PATH}" \
                    -H "Content-Type: application/json" \
                    -d "{\"fingerprint\":\"${FINGERPRINT}\",\"processing_status\":\"success\",\"processing_time\":${PROCESSING_TIME}}" \
                    --max-time 30 --retry 3 && \
                  echo "Database updated successfully for ${INPUT_PATH}" || \
                  echo "Failed to update database for ${INPUT_PATH}"
                  
                  # Clean up log file on success
                  rm -f "$TEMP_LOG_FILE"
                else
                  echo "==================== ARCHIVE PROCESS FAILED ===================="
                  echo "Archive creation failed for: $INPUT_PATH"
                  echo "Log file contents (last 50 lines):"
                  echo "--------------------"
                  
                  # Show the error logs in real-time output as well
                  if [ -f "$TEMP_LOG_FILE" ]; then
                    tail -n 50 "$TEMP_LOG_FILE"
                    echo "--------------------"
                    
                    # Capture detailed error logs for database
                    DETAILED_ERROR_MSG=$(tail -n 50 "$TEMP_LOG_FILE" | sed 's/\\/\\\\/g; s/"/\\"/g; s/$/\\n/' | tr -d '\n' | sed 's/\\n$//')
                  else
                    echo "No log file found"
                    DETAILED_ERROR_MSG="No detailed logs available"
                  fi
                  
                  # Update database with failed status and detailed error via API
                  echo "Updating database for ${INPUT_PATH} with failed status and detailed logs"
                  END_TIME=$(date +%s)
                  PROCESSING_TIME=$((END_TIME - START_TIME))
                  
                  curl -X PUT "${BACKEND_URL}/sqlite/folder_state/${INPUT_PATH}" \
                    -H "Content-Type: application/json" \
                    -d "{\"fingerprint\":\"${FINGERPRINT}\",\"processing_status\":\"failed\",\"processing_time\":${PROCESSING_TIME},\"error_message\":\"Archive creation failed\",\"detailed_error_message\":\"${DETAILED_ERROR_MSG}\"}" \
                    --max-time 30 --retry 3 && \
                  echo "Database updated with failed status for ${INPUT_PATH}" || \
                  echo "Failed to update database for ${INPUT_PATH}"
                  
                  # Clean up log file
                  rm -f "$TEMP_LOG_FILE"
                  exit 1
                fi

                # Show the current state of the database record after update
                echo "Database record state after update:"
                curl -s "${BACKEND_URL}/sqlite/folder_state/${INPUT_PATH}" \
                  --max-time 10 | jq -r '.folder_key, .processing_status, .error_message' 2>/dev/null || \
                echo "Failed to query database record"
              else
                # Handle invalid or empty folder case
                if [ -e "/data/input_path.txt" ]; then
                  INPUT_PATH=$(cat /data/input_path.txt)
                  FINGERPRINT=$(cat /data/folder_fingerprint.txt 2>/dev/null || echo "")
                  
                  echo "Skipping archive process - invalid or empty folder: ${INPUT_PATH}"
                  
                  # Create detailed error message for invalid/empty folder
                  DETAILED_ERROR_MSG="Folder validation failed:\\nFolder path: /lidar/${INPUT_PATH}\\nFolder exists: $([ -d \"/lidar/${INPUT_PATH}\" ] && echo 'true' || echo 'false')\\nFolder empty: $([ -d \"/lidar/${INPUT_PATH}\" ] && [ -z \"$(ls -A \"/lidar/${INPUT_PATH}\" 2>/dev/null)\" ] && echo 'true' || echo 'false')\\nTimestamp: $(date -u)"
                  
                  # Update database with failed status for invalid/empty folder via API
                  echo "Updating database for ${INPUT_PATH} with failed status (invalid/empty folder)"
                  
                  curl -X PUT "${BACKEND_URL}/sqlite/folder_state/${INPUT_PATH}" \
                    -H "Content-Type: application/json" \
                    -d "{\"fingerprint\":\"${FINGERPRINT}\",\"processing_status\":\"empty\",\"processing_time\":0,\"error_message\":\"Folder is invalid or empty\",\"detailed_error_message\":\"${DETAILED_ERROR_MSG}\"}" \
                    --max-time 30 --retry 3 && \
                  echo "Database updated with failed status for invalid folder ${INPUT_PATH}" || \
                  echo "Failed to update database for ${INPUT_PATH}"
                else
                  echo "No input path available"
                  exit 1
                fi
                
                exit 0
              fi
          volumeMounts:
            - name: fts-addlidar
              subPath: "fts-addlidar/LiDAR"
              mountPath: "{{ orig_dir }}"
              readOnly: true
            - name: fts-addlidar
              subPath: "fts-addlidar/LiDAR-Zips"
              mountPath: "{{ zip_dir }}"
            - mountPath: /data
              name: data
          resources:
            limits:
              cpu: "4"
              memory: "4Gi"
            requests:
              cpu: "500m"
              memory: "1Gi"
      volumes:
        - name: data
          emptyDir: {}
        - name: fts-addlidar
          persistentVolumeClaim:
            claimName: "{{ fts_addlidar_pvc_name }}"
