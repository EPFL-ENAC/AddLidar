apiVersion: batch/v1
kind: Job
metadata:
  name: "job-batch-lidar-zip-{{ timestamp }}"
  namespace: "archives"
spec:
  completions: {{ folders|length }}  # Dynamic based on folder count
  parallelism: {{ parallelism|default(4) }}
  completionMode: Indexed
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: job-name
                      operator: In
                      values:
                        - job-batch-lidar-zip-{{ timestamp }}
                topologyKey: "kubernetes.io/hostname"
      restartPolicy: Never
      initContainers:
        - name: "prepare-folders"
          image: "docker.io/library/bash"
          command:
            - "bash"
            - "-c"
            - |
              # Array of folders to process
              folders=(
              {% for folder in folders %}
                "{{ folder }}"
              {% endfor %}
              )

              # Get the current folder based on job index
              current_folder=${folders[$JOB_COMPLETION_INDEX]}

              # Write the input and output paths to files for the main container
              echo "{{ orig_dir }}/${current_folder}" > /data/input_path.txt
              echo "{{ zip_dir }}/${current_folder}.tar.gz" > /data/output_path.txt
          volumeMounts:
            - mountPath: /data
              name: data
          resources:
            limits:
              cpu: "100m"
              memory: "100Mi"
            requests:
              cpu: "10m"
              memory: "10Mi"
      containers:
        - name: "lidar-zip-job"
          image: "python:3.9-alpine"
          command:
            - "/bin/sh"
            - "-c"
            - |
              # Read the paths from the files created by the init container
              input_path=$(cat /data/input_path.txt)
              output_path=$(cat /data/output_path.txt)
              
              # Install necessary tools
              apk add --no-cache pigz sqlite

              # Create directory for output file
              mkdir -p $(dirname "$output_path")
              
              # Execute compression
              tar -C "$(dirname "$input_path")" --use-compress-program=pigz -cf "$output_path" "$(basename "$input_path")" && \
              echo "Archive created successfully: $output_path" && \
              
              # Update database
              python3 -c "import sqlite3, time; \
              db = sqlite3.connect('{{ db_path }}'); \
              db.execute('UPDATE folder_state SET archived_at = ? WHERE folder_key = ?', \
              (int(time.time()), '${input_path#{{ orig_dir }}/}')); \
              db.commit(); \
              db.close(); \
              print('Database updated for ${input_path#{{ orig_dir }}/}')" || \
              echo "Failed to update database for ${input_path#{{ orig_dir }}/}"
          volumeMounts:
            - mountPath: "{{ orig_dir }}"
              name: lidar-data
              readOnly: true
            - mountPath: "{{ zip_dir }}"
              name: lidar-zips
            - mountPath: /data
              name: data
            - mountPath: "{{ db_dir }}"
              name: db-dir
          resources:
            limits:
              cpu: "1000m"
              memory: "1Gi"
            requests:
              cpu: "100m"
              memory: "100Mi"
      volumes:
        - name: data
          emptyDir: {}
        - name: lidar-data
          persistentVolumeClaim:
            claimName: addlidar-smb-pvc
        - name: lidar-zips
          persistentVolumeClaim:
            claimName: addlidar-smb-zip-pvc
        - name: db-dir
          persistentVolumeClaim:
            claimName: addlidar-db-pvc